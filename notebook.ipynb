{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RESCON_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "64ff78cdee9e41a3b33413307061cfb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f15be6b6efee44e19056f0bb6013589a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d8a8cceaed884a649c849d62eea45024",
              "IPY_MODEL_5f376a4e73b94e61bd3ecb5d432eb1cd"
            ]
          }
        },
        "f15be6b6efee44e19056f0bb6013589a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "d8a8cceaed884a649c849d62eea45024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a51c597692ad40c18b6be69b6568ff2c",
            "_dom_classes": [],
            "description": "Epoch 0:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1563,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27f46919a7b34b268529cad51ad6844c"
          }
        },
        "5f376a4e73b94e61bd3ecb5d432eb1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bce79a314904413ea4b91401279a8427",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1563 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_802ecb4d95c04f68b8f99fdea971c0b5"
          }
        },
        "a51c597692ad40c18b6be69b6568ff2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27f46919a7b34b268529cad51ad6844c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bce79a314904413ea4b91401279a8427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "802ecb4d95c04f68b8f99fdea971c0b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSkRQDkVJBTf"
      },
      "source": [
        "# Residual Attention Network for Image Classification \r\n",
        "\r\n",
        "\r\n",
        "You must first load two files after starting the runtime from the repository. You can upload them directly from your local device or from GDrive.\r\n",
        "The files are stored in layers folder in [our repo](https://github.com/AlKun25/ResAttNet) :\r\n",
        " - [attention_module.py](https://github.com/AlKun25/ResAttNet/blob/main/layers/attention_module.py)\r\n",
        " - [basic_layers.py](https://github.com/AlKun25/ResAttNet/blob/main/layers/basic_layers.py)\r\n",
        "\r\n",
        " Note: There is a possibility of _import error_ being genrated, due to the way these two files are interlinked. Just make the required corrections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUBQ4aNirVKy",
        "outputId": "8fe30012-1277-4af1-b925-2d193bcf1875"
      },
      "source": [
        "!pip install pytorch-lightning "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/3d/fffaf4f83633552249a40d3d366f460f44539bce0592c568d8ee20d782fa/pytorch_lightning-1.1.6-py3-none-any.whl (687kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 696kB 17.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.7.0+cu101)\n",
            "Collecting fsspec[http]>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 10.1MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829kB 57.8MB/s \n",
            "\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (0.8)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (51.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.17.2)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.4)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 53.9MB/s \n",
            "\u001b[?25hCollecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296kB 52.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Building wheels for collected packages: future, PyYAML, idna-ssl\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=b96ef76954e42c93b3690e5922cf1418fa226047ca72ed19dc37bb539fdb0340\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=a2143baa6bcd3c543fb3fd246d541a2c57b5fcbce595ed8ebe610d9019d31d4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=d1bda176adf105b0a6efc8e42e7fc28c099c52856ed28e8683282af19053cfb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built future PyYAML idna-ssl\n",
            "Installing collected packages: multidict, idna-ssl, async-timeout, yarl, aiohttp, fsspec, future, PyYAML, pytorch-lightning\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 aiohttp-3.7.3 async-timeout-3.0.1 fsspec-0.8.5 future-0.18.2 idna-ssl-1.1.0 multidict-5.1.0 pytorch-lightning-1.1.6 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRuJXfZYrKUM"
      },
      "source": [
        "from pytorch_lightning.accelerators import accelerator\r\n",
        "from pytorch_lightning.core.datamodule import LightningDataModule\r\n",
        "import torch \r\n",
        "import torch.nn as nn \r\n",
        "from torch.nn import functional as F\r\n",
        "import pytorch_lightning as pl\r\n",
        "import time\r\n",
        "from torchvision.datasets import CIFAR10 # you can change this with CIFAR100, but remember to make all the corresponding changes in DataModule\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import transforms\r\n",
        "from attention_module import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajdMBPoDjEo3"
      },
      "source": [
        "# Defining DataModule for smooth use of the dataset in pytorch_lightning framework\r\n",
        "\r\n",
        "class CIFARDataModule(pl.LightningDataModule):\r\n",
        "  '''\r\n",
        "  You can use 2 different datasets in this DataModule: CIFAR10 and CIFAR100\r\n",
        "  To use a different dataset, change the import as welll as make corresponding changes in \r\n",
        "  If you change the dataset, make sure to change the value of n_classes accordingly\r\n",
        "  '''\r\n",
        "  def prepare_data(self, image_size):\r\n",
        "      # prepare transforms standard to CIFAR-10\r\n",
        "      CIFAR10(root='./data/', train=True, download=True)\r\n",
        "      CIFAR10(root='./data/', train=False, download=True)\r\n",
        "      # this depends on the model you use. It can be 224 or 32\r\n",
        "      self.image_size = image_size\r\n",
        "\r\n",
        "  def train_dataloader(self):\r\n",
        "      train_transform = transforms.Compose([  transforms.RandomHorizontalFlip(),\r\n",
        "                                              transforms.RandomCrop((32, 32), padding=4),   #left, top, right, bottom, \r\n",
        "                                              transforms.Resize(self.image_size),\r\n",
        "                                              transforms.ToTensor()\r\n",
        "                                          ])\r\n",
        "      cifar_train = CIFAR10(root='./data/', train=True, download=False, transform=train_transform)\r\n",
        "      cifar_train = DataLoader( dataset=cifar10_train, \r\n",
        "                                  batch_size=32, \r\n",
        "                                  shuffle=True, \r\n",
        "                                  num_workers=4\r\n",
        "                              )\r\n",
        "      return cifar_train\r\n",
        "\r\n",
        "  def test_dataloader(self):\r\n",
        "      test_transform = transforms.Compose([transforms.Resize(self.image_size),\r\n",
        "                                           transforms.ToTensor()])\r\n",
        "      cifar_test = CIFAR10(root='./data/', train=False,download=False,transform=test_transform)\r\n",
        "      cifar_test = DataLoader( dataset=cifar_test, \r\n",
        "                                  batch_size=20, \r\n",
        "                                  shuffle=False\r\n",
        "                              )\r\n",
        "      return cifar_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9cyuX8pOoN0"
      },
      "source": [
        "### Model architecture:\r\n",
        "There are different models available to be used in the repository, in the [models](https://github.com/AlKun25/ResAttNet/tree/main/model) folder. The models available to be used are the following:\r\n",
        "  - [Attention-56, image_size = 224](https://github.com/AlKun25/ResAttNet/blob/main/model/RAN_56_224.py)\r\n",
        "  - [Attention-92, image_size = 224](https://github.com/AlKun25/ResAttNet/blob/main/model/RAN_92_224.py)\r\n",
        "  - [Attention-92, image_size = 32](https://github.com/AlKun25/ResAttNet/blob/main/model/RAN_92_32.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eq3VRjtdJB5"
      },
      "source": [
        "# Defining model and its architecture\r\n",
        "\r\n",
        "class ResidualAttentionModel(pl.LightningModule):\r\n",
        "    '''\r\n",
        "    You have to place the model here\r\n",
        "    Replace code in this block to use different models\r\n",
        "    When you change the model, remember to change the value of image_size accordingly\r\n",
        "    '''\r\n",
        "    def __init__(self, n_classes):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1 = nn.Sequential(\r\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias = False),\r\n",
        "            nn.BatchNorm2d(64),\r\n",
        "            nn.ReLU(inplace=True)\r\n",
        "        )\r\n",
        "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "        self.residual_block1 = ResidualBlock(64, 256)\r\n",
        "        self.attention_module1 = AttentionModule_stage1(256, 256)\r\n",
        "        self.residual_block2 = ResidualBlock(256, 512, 2)\r\n",
        "        self.attention_module2 = AttentionModule_stage2(512, 512)\r\n",
        "        self.attention_module2_2 = AttentionModule_stage2(512, 512)  # tbq add\r\n",
        "        self.residual_block3 = ResidualBlock(512, 1024, 2)\r\n",
        "        self.attention_module3 = AttentionModule_stage3(1024, 1024)\r\n",
        "        self.attention_module3_2 = AttentionModule_stage3(1024, 1024)  # tbq add\r\n",
        "        self.attention_module3_3 = AttentionModule_stage3(1024, 1024)  # tbq add\r\n",
        "        self.residual_block4 = ResidualBlock(1024, 2048, 2)\r\n",
        "        self.residual_block5 = ResidualBlock(2048, 2048)\r\n",
        "        self.residual_block6 = ResidualBlock(2048, 2048)\r\n",
        "        self.mpool2 = nn.Sequential(\r\n",
        "            nn.BatchNorm2d(2048),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.AvgPool2d(kernel_size=7, stride=1)\r\n",
        "        )\r\n",
        "        self.fc = nn.Linear(2048,n_classes) # n_classes =10(CIFAR10)/100(CIFAR100)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.mpool1(out)\r\n",
        "        # print(out.data)\r\n",
        "        out = self.residual_block1(out)\r\n",
        "        out = self.attention_module1(out)\r\n",
        "        out = self.residual_block2(out)\r\n",
        "        out = self.attention_module2(out)\r\n",
        "        out = self.attention_module2_2(out)\r\n",
        "        out = self.residual_block3(out)\r\n",
        "        # print(out.data)\r\n",
        "        out = self.attention_module3(out)\r\n",
        "        out = self.attention_module3_2(out)\r\n",
        "        out = self.attention_module3_3(out)\r\n",
        "        out = self.residual_block4(out)\r\n",
        "        out = self.residual_block5(out)\r\n",
        "        out = self.residual_block6(out)\r\n",
        "        out = self.mpool2(out)\r\n",
        "        out = out.view(out.size(0), -1)\r\n",
        "        out = self.fc(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "    def cross_entropy_loss(self, logits, labels):\r\n",
        "        return F.cross_entropy(logits, labels)\r\n",
        "\r\n",
        "    def training_step(self, train_batch, batch_idx):\r\n",
        "        images, labels = train_batch\r\n",
        "        images = images.cuda()\r\n",
        "        labels = labels.cuda()\r\n",
        "        logits = self.forward(images)\r\n",
        "        loss = self.cross_entropy_loss(logits, labels)\r\n",
        "        self.log('train_loss', loss)\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def test_step(self,test_batch, batch_idx):\r\n",
        "        x, y = test_batch\r\n",
        "        y_hat = self.forward(x)\r\n",
        "        loss = self.cross_entropy_loss(y_hat, y)\r\n",
        "        self.log('test_loss', loss)\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def configure_optimizers(self):\r\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=0.1, momentum=0.9, nesterov=True, weight_decay=0.0001)\r\n",
        "        return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHrgFHyFdiX4"
      },
      "source": [
        "class LitProgressBar(pl.callbacks.ProgressBar): # Progress bar for seeing the progress during train and test phase\r\n",
        "\r\n",
        "    def init_train_tqdm(self):\r\n",
        "        bar = super().init_train_tqdm()\r\n",
        "        bar.set_description('running training ...')\r\n",
        "        bar.leave = True\r\n",
        "        return bar\r\n",
        "\r\n",
        "    def init_test_tqdm(self):\r\n",
        "        bar = super().init_test_tqdm()\r\n",
        "        bar.set_description('running testing ...')\r\n",
        "        bar.leave = True\r\n",
        "        return bar\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "64ff78cdee9e41a3b33413307061cfb7",
            "f15be6b6efee44e19056f0bb6013589a",
            "d8a8cceaed884a649c849d62eea45024",
            "5f376a4e73b94e61bd3ecb5d432eb1cd",
            "a51c597692ad40c18b6be69b6568ff2c",
            "27f46919a7b34b268529cad51ad6844c",
            "bce79a314904413ea4b91401279a8427",
            "802ecb4d95c04f68b8f99fdea971c0b5"
          ]
        },
        "id": "3Hh3at6zdkAU",
        "outputId": "0181f65f-708b-407e-b2b7-45571ecad313"
      },
      "source": [
        "bar = LitProgressBar(refresh_rate=50) # keep refresh rate more than 20\r\n",
        "\r\n",
        "data_module = CIFARDataModule(image_size=224) # image_size needs to change when you change model\r\n",
        "data_module.prepare_data() # loads data onto Colab's runtime\r\n",
        "test_data = data_module.test_dataloader()\r\n",
        "train_data = data_module.train_dataloader()\r\n",
        "\r\n",
        "model = ResidualAttentionModel(n_classes=10) # n_classes need to change when you change dataset\r\n",
        "\r\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=-1, callbacks=[bar], accelerator='ddp', progress_bar_refresh_rate=50)\r\n",
        "\r\n",
        "trainer.fit(model, train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "\n",
            "   | Name                | Type                   | Params\n",
            "----------------------------------------------------------------\n",
            "0  | conv1               | Sequential             | 9.5 K \n",
            "1  | mpool1              | MaxPool2d              | 0     \n",
            "2  | residual_block1     | ResidualBlock          | 74.1 K\n",
            "3  | attention_module1   | AttentionModule_stage1 | 1.8 M \n",
            "4  | residual_block2     | ResidualBlock          | 377 K \n",
            "5  | attention_module2   | AttentionModule_stage2 | 5.4 M \n",
            "6  | attention_module2_2 | AttentionModule_stage2 | 5.4 M \n",
            "7  | residual_block3     | ResidualBlock          | 1.5 M \n",
            "8  | attention_module3   | AttentionModule_stage3 | 15.1 M\n",
            "9  | attention_module3_2 | AttentionModule_stage3 | 15.1 M\n",
            "10 | attention_module3_3 | AttentionModule_stage3 | 15.1 M\n",
            "11 | residual_block4     | ResidualBlock          | 6.0 M \n",
            "12 | residual_block5     | ResidualBlock          | 8.7 M \n",
            "13 | residual_block6     | ResidualBlock          | 8.7 M \n",
            "14 | mpool2              | Sequential             | 4.1 K \n",
            "15 | fc                  | Linear                 | 20.5 K\n",
            "----------------------------------------------------------------\n",
            "83.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "83.2 M    Total params\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64ff78cdee9e41a3b33413307061cfb7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6soNz2F362D"
      },
      "source": [
        "# Testing the trained model\r\n",
        "trainer.test(test_dataloaders=test_data, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aQiskiSZ8Us"
      },
      "source": [
        "# Saving the trained model as checkpoint\r\n",
        "trainer.save_checkpoint('CIFAR10_92.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4x2Dt8haFFh"
      },
      "source": [
        "# Loading tensorboard\r\n",
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys6mzAdlfi-o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}